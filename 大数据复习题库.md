# 一、 单项选择题

1. 在 Hadoop 3.x 版本中，HDFS 默认使用哪个端口号作为 NameNode 的 Web 界面端口？（ ） 

   A. 50070 

   B. 9870 

   C. 8088 

   D. 8020

2. HDFS 的设计目标是支持哪种类型的数据访问模式？（ ） 

   A. 大量小文件的随机读写 		

   B. 一次写入，多次读取（Write-Once, Read-Many） 

   C. 高并发的随机更新 			

   D. 实时流式处理

3. 在 Hadoop 集群启动脚本中，用于启动所有 NameNode、DataNode、ResourceManager 和 NodeManager 进程的命令是？（ ） 

   A. start-namenode.sh 		 

   B. start-all.sh（Hadoop 2.x 常用） 

   C. start-dfs.sh 				

   D. start-dfs.sh 和 start-yarn.sh 组合，或使用 start-all.sh (如果配置了)

4. 在 MapReduce 编程模型中，哪个阶段负责将 Mapper 输出的键值对进行分组和排序？（ ） 

   A. Map 	

   B. Shuffle 	

   C. Reduce 	

   D. InputFormat

5. 在 Hive 中，如果需要创建一个表，并指定数据存储在 HDFS 的某个特定目录，应该使用哪个关键字？（ ） 

   A. ROW FORMAT 	

   B. STORED AS 	

   C. LOCATION 	

   D. PARTITIONED BY

6. HBase 是一个基于 HDFS 的（ ）数据库。 

   A. 关系型 	

   B. 图形 	

   C. 列式（Column-oriented/Key-Value） 	

   D. 文档型

7. 使用 IDEA 进行 MapReduce 程序开发时，通常需要将项目打包成什么格式才能提交到 Hadoop 集群运行？（ ） 

   A. .zip 	

   B. .war 	

   C. .jar 	

   D. .class

8. 当 DataNode 发生故障时，HDFS 能够通过什么机制来保证数据不丢失？（ ） 

   A. NameNode 备份 	

   B. 多副本（Replication）机制 

   C. Checkpoint 机制 	

   D. 快照（Snapshot）

9. 在 Hadoop 集群运维中，如果 DataNode 进程启动失败，首先需要检查的文件是哪个？（ ） 

   A. yarn-site.xml 		

   B. DataNode 的日志文件 

   C. mapred-site.xml	   

   D. /etc/hosts

10. 以下哪个命令是用于查看 HDFS 根目录下文件的？（ ） 

    A. hdfs fs -ls / 		 

    B. hadoop fs -ls / 

    C. hdfs dfs -ls / 		

    D. 以上都是常用且正确的命令

11. Hadoop集群中，NameNode的主要作用是（）

    A. 存储实际数据

    B. 管理HDFS的命名空间和元数据

    C. 执行MapReduce任务

    D. 协调各节点之间的通信

12. HDFS默认的数据块大小是（）

    A. 64MB

    B. 128MB

    C. 256MB

    D. 512MB

13. 在Hadoop集群部署中，以下哪个文件用于配置集群的所有DataNode节点（）

    A. hadoop-env.sh

    B. core-site.xml

    C. workers

    D. hdfs-site.xml

14. MapReduce中，Shuffle过程发生在（）阶段

    A. Map之前

    B. Map和Reduce之间

    C. Reduce之后

    D. 只在Map阶段

15. Hive的数据存储默认位于（）

    A. 本地文件系统

    B. HDFS

    C. MySQL数据库

    D. HBase 

16. HBase是基于（）模型的NoSQL数据库

    A. 关系型

    B. 文档型

    C. 列族

    D. 图数据库

17. 以下哪个命令可以查看HDFS集群的健康状态（）

    A. hadoop fs -ls

    B. hdfs dfsadmin -report

    C. start-dfs.sh

    D. jps

18. 在Java API中，以下哪个类用于读取HDFS文件（）

    A. FileOutputStream

    B. FSDataInputStream

    C. BufferedReader

    D. FileReader

19. Hadoop集群启动顺序，正确的是（）

    A. 先启动YARN，再启动HDFS

    B. 先启动HDFS，再启动YARN

    C. 可以任意顺序启动

    D. 必须同时启动

20. Hive中，以下哪个不是合法的数据类型（）

    A. STRING

    B. INT

    C. ARRAY

    D. BLOB

21. Hadoop 集群中负责资源管理与任务调度的组件是：

    A. NameNode

    B. DataNode

    C. ResourceManager

    D. NodeManager

22. HDFS 中用于存储文件元数据（如文件块位置）的角色是：

    A. SecondaryNameNode

    B. NameNode

    C. DataNode

    D. JobTracker

23. 在 Hadoop 集群部署中，以下哪个是 SSH 免密登录的作用？

    A. 提高 HDFS 读写速度

    B. 实现节点间自动任务分发

    C. 允许节点间无密码执行命令

    D. 保证 NameNode 高可用

24. MapReduce 中，负责把 Mapper 输出按 key 分组的是：

    A. InputFormat

    B. Combiner

    C. Partitioner

    D. Shuffle

25. 在 IDEA 中编写 Hadoop 程序时，需要引入的依赖一般为：

    A. hadoop-common/hadoop-hdfs

    B. mysql-connector-java

    C. spark-core

    D. hive-jdbc

26. 向 HDFS 上传一个文件的命令是：

    A. hdfs dfs -get

    B. hdfs dfs -put

    C. hdfs dfs -touchz

    D. hdfs dfs -du

27. 启动 HBase 服务的命令通常是：

    A. start-dfs.sh

    B. start-yarn.sh

    C. start-hbase.sh

    D. hbase shell

28. Hive 的默认运行模式是：

    A. 本地模式

    B. MapReduce 模式

    C. Tez 模式

    D. Spark 模式

29. 以下哪个是 Java HDFS API 中常用的文件系统对象？

    A. FileSystem

    B. ConfigurationManager

    C. HadoopFS

    D. HdfsReader

30. 以下哪个操作属于 Hadoop 集群运维工作？

    A. Java 编写 Mapper 程序

    B. HDFS 文件读取

    C. 检查 NameNode 状态和 DataNode 存活

    D. 编写 Hive SQL

31. 下列不属于Hadoop集群常见部署模式的是（  ）

    A. 单机模式  			B. 伪分布式模式  

    C. 完全分布式模式  	    D. 云原生模式

32. 在Hadoop集群部署过程中，以下哪个配置文件用于指定Hadoop的环境变量（如JDK路径）（  ）

    A. core-site.xml 	 B. hadoop-env.sh  

    C. hdfs-site.xml  	D. mapred-site.xml

33. HDFS中负责存储实际数据块的角色是（  ）  

    A. NameNode  			  B. DataNode  

    C. SecondaryNameNode  	D. ResourceManager

34. MapReduce编程模型中，负责将输入数据分割成数据块的阶段是（  ）

    A. Map阶段  B. Shuffle阶段  

    C. Reduce阶段  D. InputFormat阶段

35. 使用IDEA开发Hadoop Java API程序时，需要导入的核心依赖包所属的组织是（  ）   

    A. org.apache.spark  B. org.apache.hadoop  

    C. org.apache.hive  D. org.apache.hbase

36. Hive的核心功能是（  ）

    A. 数据存储  B. 数据计算  

    C. 将SQL转换为MapReduce任务  D. 集群监控

37. HBase中用于唯一标识一行数据的是（  ）

    A. 列族  B. 列  

    C. 行键（RowKey）  D. 时间戳

38. 在Hadoop集群运维中，用于启动HDFS集群的命令是（  ）

    A. start-dfs.sh  B. start-yarn.sh  

    C. start-all.sh  D. hdfs start

39. 下列关于HDFS的描述，错误的是（  ）

    A. HDFS适合存储大文件 			 B. HDFS支持随机写操作  

    C. HDFS通过副本机制保证数据可靠性   D. HDFS采用主从架构

40. Hive中创建表时，用于指定表数据存储位置的关键字是（  ）

    A. location  B. path  

    C. directory  D. storage

41. Hadoop集群部署前不需要做以下哪项准备工作？ 

    A. 配置SSH免密登录
    B. 安装JDK
    C. 关闭防火墙
    D. 安装MySQL数据库

42. HDFS默认的块大小是： 

    A. 64MB
    B. 128MB
    C. 256MB
    D. 512MB

43. 格式化NameNode的命令是： 

    A. hdfs datanode -format
    B. hadoop namenode -format
    C. hdfs namenode -format
    D. hadoop format

44. 启动Hadoop集群所有服务的命令是： 

    A. start-dfs.sh
    B. start-yarn.sh
    C. start-all.sh
    D. hadoop-start.sh

45. 在MapReduce中，哪个阶段负责将相同key的数据聚合在一起？ 

    A. Map
    B. Shuffle
    C. Reduce
    D. Combiner

46. HDFS的默认副本数是： 

    A. 1
    B. 2
    C. 3
    D. 4

47. 以下哪个不是Hadoop生态圈的组件？ 

    A. Hive
    B. HBase
    C. Spark
    D. MySQL

48. 在Hadoop伪分布式部署中，所有守护进程运行在： 

    A. 多台机器
    B. 一台机器
    C. 虚拟机
    D. 容器

49. 以下哪个命令用于查看HDFS文件列表？ 

    A. hdfs dfs -ls
    B. hdfs dfs -cat
    C. hdfs dfs -mkdir
    D. hdfs dfs -rm

50. Hive的作用是： 

    A. 分布式计算
    B. 分布式存储
    C. 数据仓库和SQL查询
    D. 实时数据处理

51. Hadoop的核心组件不包括以下哪一项？

    A. HDFS	B. MapReduce

    C. YARN	D. MySQL

52. 以下哪个端口不是Hadoop集群中常用的默认端口？

    A. 50070 (NameNode Web UI)

    B. 8088 (ResourceManager Web UI)

    C. 3306 (MySQL)

    D. 9000 (HDFS RPC)

53. 在HDFS中，负责存储文件数据的节点是？

    A. NameNode

    B. DataNode

    C. SecondaryNameNode

    D. ResourceManager

54. MapReduce编程模型中，Map阶段的输出会经过哪个过程后进入Reduce阶段？

    A. 排序和分区

    B. 压缩和解压

    C. 加密和解密

    D. 复制和备份

55. 在Hive中，将SQL语句转换为MapReduce任务的组件是？

    A. Driver

    B. Metastore

    C. Compiler

    D. Execution Engine

56. HBase是基于哪种数据库模型构建的？

    A. 关系型数据库

    B. 键值对存储

    C. 文档数据库

    D. 图数据库

57. 在IDEA中创建Maven项目时，用于管理Hadoop依赖的配置文件是？

    A. pom.xml	B. build.gradle

    C. settings.xml	D. application.properties

58. 以下哪个命令用于在HDFS上创建一个新目录？

    A. hdfs dfs -mkdir /test	B. hdfs dfs -create /test

    C. hdfs dfs -newdir /test	D. hdfs dfs -add /test

59. 在YARN中，负责管理应用程序资源分配的组件是？

    A. NodeManager	B. ApplicationMaster

    C. ResourceManager	D. Container

60. 当Hadoop集群中的DataNode发生故障时，HDFS会自动？

    A. 停止整个集群
    
    B. 从其他DataNode复制数据块副本
    
    C. 删除故障节点上的所有数据
    
    D. 等待管理员手动恢复

 

# 多选题

Hadoop集群的角色包括（）

 

A. NameNode

B. DataNode

C. ResourceManager

D. NodeManager

E. SecondaryNameNode

 

 

以下属于HDFS特点的有（）

 

A. 高容错性

B. 适合大文件存储

C. 支持数据的随机读写

D. 数据自动备份

E. 流式数据访问

 

 

MapReduce程序的核心组件包括（）

 

A. Mapper

B. Reducer

C. Driver

D. Combiner

E. Partitioner

 

 

Hadoop集群运维中常用的监控指标有（）

 

A. HDFS磁盘使用率

B. DataNode存活状态

C. NameNode堆内存使用情况

D. MapReduce任务执行时间

E. 网络带宽占用

 

 

在IDEA中开发Hadoop程序需要添加的依赖包括（）

 

A. hadoop-common

B. hadoop-hdfs

C. hadoop-mapreduce-client-core

D. mysql-connector-java

E. hadoop-client

 

# 二、判断题

 

（ ）NameNode 负责存储文件的实际数据。

 

（ ）HDFS 适合存储大文件，不适合大量的小文件。

 

（ ）MapReduce 程序必须包含 Mapper 和 Reducer 两部分。

 

（ ）Hive 可以自动将 SQL 转换为 MapReduce 或 Tez 作业执行。

 

（ ）HBase 擅长海量数据的随机读写。

 

（）Hadoop集群中可以有多个NameNode同时工作在Active状态。

（）HDFS的副本数可以通过dfs.replication参数配置。

（）MapReduce中，Reduce任务的数量由输入数据的splits数量决定。

（）SecondaryNameNode是NameNode的热备份节点。

（）Hive支持标准SQL的所有语法。

（）HBase适合存储大量小文件。

（）在Java API中操作HDFS需要导入org.apache.hadoop.fs包。

（）Hadoop集群停止时应先停止YARN，再停止HDFS。

（）HDFS的fsck命令可以用来检查文件系统的健康状况。

（）Hive的元数据默认存储在Derby数据库中。

 

 

# 二、 填空题

 

Hadoop 集群中，通常用于资源调度的组件是 ***\*YARN\****。

 

在 MapReduce 编程中，用户自定义的 Mapper 类需要继承自 org.apache.hadoop.mapreduce.Mapper，其输出的 Key 和 Value 必须是 Hadoop 自定义的 ***\*Writable\**** 类型。

 

要使用 Java API 将本地文件 /home/data/input.txt 上传到 HDFS 的 /user/test 目录下，需要使用 FileSystem 类的 ***\*copyFromLocalFile\**** 方法。

 

在 Hive 中，如果一张表的元数据被删除，但 HDFS 上的数据文件仍然保留，那么这张表是 ***\*外部表（EXTERNAL TABLE）\****。

 

HBase 的数据存储结构是 ***\*表（Table）\****，它由 ***\*行键（Row Key）、列族（Column Family） 和 时间戳（Timestamp）\**** 唯一确定一个单元格（Cell）。

 

1. Hadoop集群的核心组件包括________、________和YARN。

 

2. HDFS默认的数据块大小是________MB（Hadoop 2.x版本），默认的副本数是________个。

 

3. MapReduce程序中，Map函数的输入数据类型是________，输出数据类型是________。

 

4. Hive元数据默认存储在________数据库中，用于记录表结构、数据位置等信息。

 

5. HBase是基于________模型的分布式数据库，其表结构由________和列组成。

 

6. 在完全分布式Hadoop集群中，为实现节点间免密登录，需要生成________密钥和________密钥。

 

1. Hadoop的三大核心组件是________、***\*和\****。
2. HDFS采用________架构，由________和________组成。
3. 在Hadoop完全分布式部署中，通常需要配置________文件来指定从节点主机名。
4. 格式化HDFS的命令是________。
5. HDFS默认Web界面访问端口是________，YARN的Web界面端口是________。
6. MapReduce作业中，________阶段负责数据的分片和映射，________阶段负责数据的聚合和输出。
7. Hive的元数据默认存储在________数据库中。
8. HBase是一个________型的分布式数据库，适用于________查询场景。
9. 在Hadoop集群中，________负责资源管理和任务调度。
10. 为了提高Hadoop集群的可靠性，可以配置________来实现NameNode的高可用。

 

1. Hadoop集群通常有三种部署模式：______、伪分布式模式和完全分布式模式。
2. 在HDFS中，文件被分割成固定大小的数据块，默认大小为______MB。
3. MapReduce程序的两个核心函数 是 ***\**和\**\***。
4. 在IDEA中运行MapReduce程序前，需要配置______插件来支持Hadoop开发。
5. Hive的元数据通常存储在______数据库中（默认）。
6. HBase的底层存储依赖于______文件系统。
7. 查看HDFS磁盘使用情况的命令是______。
8. 在YARN中，每个应用程序的运行实例称为______。
9. 启动Hadoop集群的命令是______（在主节点执行）。
10. 在HBase Shell中，创建表的命令是______。

 

 

# 三、 简答题

 

简述Hadoop伪分布式部署和完全分布式部署的主要区别。

列举HDFS的三个主要配置文件及其作用。

简述MapReduce的基本执行流程（包括Map、Shuffle、Reduce阶段）。

说明Hive和HBase的主要区别及各自的应用场景。

Hadoop集群日常运维中需要检查哪些关键指标？

 

***\*描述 Hadoop 伪分布式 集群部署的关键步骤。（重点/偏运维）\****

**·** ***\*评分要点：\**** 至少提及 4 个核心步骤，如：环境准备、配置文件修改、格式化、启动验证。

**·** ***\*参考答案：\****

**1.** ***\*环境准备：\**** 安装并配置好 Java 环境（如 JDK 1.8+）。

**2.** ***\*配置 SSH 免密登录：\**** 配置本机到本机的 SSH 免密登录，这是启动脚本的基础。

**3.** ***\*修改核心配置文件：\**** 主要修改 core-site.xml 和 hdfs-site.xml，配置 NameNode 的地址和 HDFS 的数据目录等。

**4.** ***\*格式化 NameNode：\**** 首次启动前，执行 hdfs namenode -format。

**5.** ***\*启动与验证：\**** 执行 start-dfs.sh 启动 HDFS，并通过 jps 命令查看 NameNode 和 DataNode 进程是否成功启动。

***\*2. 简述 HDFS 的三个核心角色（进程）及其主要职责。（偏运维）\****

**·** ***\*评分要点：\**** 明确NameNode、DataNode、Secondary NameNode/JournalNode（Hadoop 3.x HA）三者的名称和各自最主要的功能。

**·** ***\*参考答案：\****

**1.** ***\*NameNode：\**** HDFS 的***\*主控节点\****，负责存储文件系统的***\*元数据\****（Metadata），如文件名、目录结构、文件块的映射信息等。它是整个系统的***\*单点\****（HA除外）。

**2.** ***\*DataNode：\**** HDFS 的***\*工作节点\****，负责存储实际的***\*数据块\****（Block），并执行来自客户端的读写请求。它会定期向 NameNode 发送***\*心跳\****和***\*块报告\****。

**3.** ***\*Secondary NameNode/JournalNode：\**** 在非 HA 模式下，***\*Secondary NameNode\**** 辅助 NameNode 合并编辑日志（EditLog）生成新的镜像文件（FsImage）。在 HA 模式下，***\*JournalNode\**** 负责同步主备 NameNode 的元数据，确保数据一致性。

***\*3. MapReduce 程序的执行过程分为哪两个主要阶段？并简单说明每个阶段主要负责的功能。（偏原理）\****

**·** ***\*评分要点：\**** 明确 Map 和 Reduce 两个阶段，并简述各自功能。

**·** ***\*参考答案：\****

**1.** ***\*Map 阶段（映射阶段）：\****

**§** ***\*功能：\**** 读取输入数据，并对每一条记录进行***\*处理\****（如过滤、转换），将其转化为一组***\*中间键值对\**** $\langle K_2, V_2 \rangle$ 输出。

**§** ***\*主要组件：\**** InputFormat、Mapper。

**2.** ***\*Reduce 阶段（规约阶段）：\****

**§** ***\*功能：\**** 接收 Map 阶段的输出，经过 ***\*Shuffle\**** 过程（排序、分组）后，对具有***\*相同 Key\**** 的所有 Value 进行***\*聚合、统计或计算\****，最终输出最终结果 $\langle K_3, V_3 \rangle$。

**§** ***\*主要组件：\**** Shuffle & Sort、Reducer、OutputFormat。

 

 

1. 简述Hadoop伪分布式集群的部署步骤（至少包含5个关键步骤）。

 

2. 简述HDFS读取文件的基本流程。

 

3. 简述MapReduce的执行流程，包括主要阶段及各阶段的核心作用。

 

4. 简述Hive和HBase的主要区别及适用场景。

 

1. 简述Hadoop集群部署前的准备工作。
2. 简述HDFS的优缺点。
3. 简述MapReduce的工作原理。
4. 简述Hive与传统关系型数据库的区别。
5. 简述如何扩容Hadoop集群（增加DataNode节点）。

 

1. 简述Hadoop集群中NameNode和DataNode的作用及区别。
2. 描述HDFS写数据的基本流程（从客户端到最终存储）。
3. 什么是MapReduce的Shuffle阶段？它包含哪些主要过程？
4. 列举Hive与传统关系型数据库的三个主要区别。
5. 简述HBase的RowKey设计原则及其重要性。

 

·  ***\*简述 Hadoop 集群部署的主要步骤（至少 4 步）。\****

·  ***\*HDFS 的主要特点有哪些？（至少 3 点）\****

·  ***\*简述 MapReduce 的执行流程。\****

·  ***\*简述 Hive 的作用及适用场景。\****

·  ***\*说明 HBase 适合处理哪类数据，并举一个典型应用场景。\****

 

 

# 四、 实操题

 

***\*1\*******\*. HDFS 命令行操作题（20 分）\****

假设您的 Hadoop 集群已经正常运行。请写出完成下列操作的 ***\*HDFS 命令行\****：

1. 在 HDFS 的 / 目录下创建一个名为 data 的文件夹。

**o** ***\*命令：\**** hdfs dfs -mkdir /data

2. 将本地 Linux 系统下的 /home/local/log.txt 文件上传到 HDFS 的 /data 文件夹中。

**o** ***\*命令：\**** hdfs dfs -put /home/local/log.txt /data

3. 查看 HDFS 的 /data 目录下有哪些文件。

**o** ***\*命令：\**** hdfs dfs -ls /data

4. 将 HDFS 上的 /data/log.txt 文件内容下载到本地的 /tmp 目录下，并重命名为 download.txt。

**o** ***\*命令：\**** hdfs dfs -get /data/log.txt /tmp/download.txt

***\*2. Hive 基础查询语句题（20 分）\****

假设您在 Hive 中已经有一个名为 student 的表，包含以下字段：id(INT), name(STRING), score(INT), class(STRING)。

请写出完成下列操作的 ***\*HiveQL 语句\****：

1. 查询所有学生的姓名（name）和分数（score），并按照分数降序排列。

**o** ***\*语句：\**** SELECT name, score FROM student ORDER BY score DESC;

2. 查询每个班级（class）的***\*平均分数\****，并将结果命名为 avg_score。

**o** ***\*语句：\**** SELECT class, AVG(score) AS avg_score FROM student GROUP BY class;

3. 查询分数***\*大于 80 分\****的学生总人数。

**o** ***\*语句：\**** SELECT COUNT(*) FROM student WHERE score > 80;

4. 修改表 student 的名称为 student_info。

**o** ***\*语句：\**** ALTER TABLE student RENAME TO student_info;

 

 

1. 请写出在Hadoop集群运维中，以下操作对应的命令：

​    

（1）关闭HDFS集群

​    

（2）查看HDFS中根目录下的文件列表

​    

（3）在HDFS中创建名为“student”的目录

​    

（4）查看DataNode节点的运行状态

   

 

2. 使用Java API编写一段简单代码，实现将本地路径“/home/user/data.txt”的文件上传到HDFS的“/user/hadoop/input”目录下（需写出核心代码及关键注释）。

 

 

***\*1. HDFS Java API编程题（10分）\****

编写Java程序，实现以下功能：

· 在HDFS上创建一个新文件/user/test/hello.txt

· 向文件中写入内容Hello Hadoop!

· 关闭文件流

 

***\*2. MapReduce编程题（10分）\****

补充完整以下Java代码，实现WordCount功能：
***\*Mapper类：\****

 

***\*Reducer类：\****

 

***\*3. Hive操作题（10分）\****

给定一个员工表employee，结构如下：

employee(id int, name string, dept string, salary int)

请写出Hive SQL语句完成以下操作：

· 创建表employee

· 向表中插入一条记录：(1, '张三', '技术部', 8000)

· 查询每个部门的平均工资

· 查询工资高于5000的员工姓名和部门

 

 

1. ***\*集群部署与运维\****​假设你需要在一台Linux服务器上搭建一个伪分布式Hadoop集群（版本3.3.4），请写出以下步骤的关键命令或操作：（1）解压安装包到指定目录（如/usr/local/hadoop）；（2）配置环境变量（HADOOP_HOME和PATH）；（3）修改core-site.xml配置文件（设置默认文件系统和临时目录）；（4）格式化NameNode；（5）启动HDFS服务并验证是否成功（通过jps命令查看进程）。

**2.** ***\*HDFS文件操作\****使用HDFS Shell命令完成以下任务：（1）在HDFS根目录下创建名为"exam"的目录；（2）将本地当前目录下的"data.txt"文件上传到HDFS的"/exam"目录；（3）查看"/exam/data.txt"文件的内容；（4）统计"/exam"目录下所有文件的总大小（以字节为单位）；（5）删除"/exam"目录及其所有内容。

**3.** ***\*Hive基础操作\****已知Hive已安装并启动，Metastore使用Derby数据库。请写出以下操作的HQL语句：（1）创建一个名为"student"的表，包含字段id(int)、name(string)、age(int)，按id分区；（2）向"student"表中插入一条记录：(1, "Alice", 20)；（3）查询年龄大于18岁的学生信息；（4）计算学生平均年龄；（5）删除"student"表。

 

***\*1. 写出 Hadoop 集群部署基础配置的关键步骤（填写配置项即可）\****

包括但不限于：

· JDK 配置

· SSH 免密登录

· core-site.xml

· hdfs-site.xml

· yarn-site.xml

· workers/slaves 文件
（写主要内容即可）



------



***\*2. 写出常用 HDFS 命令 5 条并说明其功能\****

例如：创建目录、上传文件、查看文件、删除文件等。



------



***\*3. 编写一个简单的 MapReduce WordCount 示例伪代码\****

要求：

· Mapper 输入 Text，输出 <word,1>

· Reducer 对 key 求和
（可写伪代码或简介代码框架）



------



***\*4. 给出 Hive 中建表 + 加载数据 + 简单查询的 SQL 示例\****

至少包含：

· 创建表

· 加载本地文件

· group by 查询

 

 

 

***\*1. Hadoop集群部署配置题（10分）\****

请完成以下配置文件的关键配置项填写：

***\*core-site.xml配置：\****

***\*hdfs-site.xml配置：\****

 

***\*2. HDFS命令操作题（10分）\****

写出完成以下操作的HDFS命令：

(1) 在HDFS上创建目录 /user/data

(2) 上传本地文件 test.txt 到HDFS的 /user/data 目录

(3) 查看HDFS上 /user/data 目录下的所有文件

(4) 下载HDFS上的 /user/data/test.txt 到本地 /home 目录

(5) 删除HDFS上的 /user/data/test.txt 文件

***\*3. MapReduce编程题（10分）\****

请补全以下WordCount程序的Mapper和Reducer代码框架：

 