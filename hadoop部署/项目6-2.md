# 基于 HBase 存储电影网站用户影评分析结果实操教程

本教程聚焦 “HBase 分布式数据库存储电影影评分析结果”，从基础认知、环境搭建到实操落地，全程围绕实操步骤与效果验证，帮助学习者掌握 HBase 在海量数据存储中的核心应用，实现电影影评分析结果的高效存储与管理。

## 一、教程核心目标

1. 理解 HBase 的核心特性、数据模型与系统架构，明确其与传统数据库的差异。
2. 掌握 ZooKeeper 集群部署与 HBase 集群安装配置，能独立完成分布式环境搭建。
3. 熟练运用 HBase Shell 命令实现表的创建、数据增删查改等基础操作。
4. 学会使用 HBase Java API 进行表设计与数据操作，落地电影影评分析结果存储全流程。

## 二、前置准备

1. 环境要求：Linux 系统（推荐 CentOS 7）、Hadoop 3.x 集群、JDK 1.8、HBase 2.5.7、ZooKeeper 3.8.3。
2. 工具准备：Xftp（文件传输）、Xshell（远程连接）、IntelliJ IDEA（Java 开发）、Web 浏览器（HBase 集群监控）。
3. 数据准备：电影影评分析结果数据（4 类核心数据：评分次数 Top10 电影、不同性别评分 Top10 电影、指定电影各年龄段平均评分、各类型评分 Top5 电影），已存储在 HDFS 指定目录。

## 三、HBase 基础认知（实操前必懂）

### 1. 核心概念与特点

HBase 是基于 Hadoop 的分布式列式非关系数据库，是 BigTable 的开源实现，弥补了 HDFS 无法随机访问数据的缺陷。其核心特点包括：

- 海量存储：支持 TB/PB 级数据存储，适配大规模数据集。
- 面向列：按列族存储数据，查询时仅读取所需列，效率更高。
- 多版本：同一单元格支持多版本数据存储，通过时间戳区分。
- 高可靠性：依赖 HDFS 存储数据，结合 ZooKeeper 实现高可用。
- 易扩展性：支持集群横向扩展，应对数据量增长需求。

### 2. 与传统关系数据库的关键差异

| 对比维度 | HBase                                     | 传统关系数据库（如 MySQL）         |
| -------- | ----------------------------------------- | ---------------------------------- |
| 数据模型 | 列式存储，行键 + 列族 + 列限定符 + 时间戳 | 行式存储，固定表结构               |
| 数据类型 | 无特定类型，均以 Byte [] 存储             | 支持多种数据类型（int、string 等） |
| 操作方式 | 主要支持单行读写、扫描，无复杂 JOIN       | 支持复杂 SQL 查询、多表 JOIN 等    |
| 索引支持 | 仅行键索引，依赖行键排序查询              | 支持主键、二级索引等复杂索引       |
| 可扩展性 | 分布式架构，横向扩展能力强                | 单表扩展有限，需分库分表           |
| 适用场景 | 海量数据存储、高并发读写、离线批处理      | 结构化数据存储、复杂查询、实时事务 |

### 3. 核心数据模型

HBase 数据模型由行键、列族、列限定符、时间戳、值 5 个核心组件构成：

- 行键（Row Key）：表的唯一标识，类似主键，按字典序排序存储。
- 列族（Column Family）：列的集合，创建表时需预先定义，同一列族的列存储属性一致。
- 列限定符（Column Qualifier）：列族下的具体列，可动态添加，无需预先定义。
- 时间戳（Timestamp）：记录数据版本，默认自动生成，支持多版本数据存储。
- 值（Value）：存储的实际数据，以字节数组形式存储。

### 4. 系统架构与读写流程

#### （1）系统架构

HBase 采用主从架构，核心组件包括：

- 客户端：发起数据读写请求，通过 ZooKeeper 获取集群元数据。
- ZooKeeper：协调集群，管理 Master 选举、RegionServer 状态监控。
- HMaster：集群主节点，负责 Region 分配、表结构管理等。
- HRegionServer：存储和管理 HRegion，处理具体数据读写请求。
- HRegion：表的分区单元，每个 Region 对应一段行键范围的数据。
- HStore：每个列族对应一个 HStore，包含 MemStore（内存缓存）和 StoreFile（磁盘文件，即 HFile）。

#### （2）核心流程

- 写流程：客户端→ZooKeeper 获取.meta. 表位置→查询目标 Region 所在 RegionServer→先写 HLog（日志）→再写 MemStore→MemStore 满后刷写到 StoreFile。
- 读流程：客户端→ZooKeeper 获取.meta. 表位置→定位目标 RegionServer→先查 MemStore，再查 StoreFile→合并结果返回。

## 四、分布式环境搭建实操

### 1. ZooKeeper 集群部署（HBase 高可用依赖）

ZooKeeper 用于协调 HBase 集群，避免 Master 单点故障，需部署奇数节点集群（此处以 3 节点为例：slave1、slave2、slave3）。

#### 操作步骤

1. 下载 ZooKeeper 3.8.3 安装包，通过 Xftp 上传至 slave1 的 /opt/apps 目录。

2. 解压安装包到 /usr/local：`tar -zxvf apache-zookeeper-3.8.3-bin.tar.gz -C /usr/local/`。

3. 配置环境变量：编辑 /etc/profile，添加`export ZOOKEEPER_HOME=/usr/local/apache-zookeeper-3.8.3-bin`和`export PATH=$PATH:$ZOOKEEPER_HOME/bin`，执行`source /etc/profile`生效。

4. 修改配置文件：

   - 进入 ZooKeeper 配置目录：`cd $ZOOKEEPER_HOME/conf`。

   - 复制模板文件：`cp zoo_sample.cfg zoo.cfg`。

   - 编辑 zoo.cfg：设置数据存储目录`dataDir=/usr/local/apache-zookeeper-3.8.3-bin/data`，添加集群节点配置：

     ```plaintext
     server.1=slave1:2888:3888
     server.2=slave2:2888:3888
     server.3=slave3:2888:3888
     ```

5. 创建 myid 文件：在 dataDir 目录下创建 myid 文件，slave1 写入 “1”、slave2 写入 “2”、slave3 写入 “3”。

6. 分发配置：将 slave1 的 ZooKeeper 目录与 /etc/profile 文件分发至 slave2、slave3，刷新环境变量。

7. 启动集群：在 3 个节点分别执行`zkServer.sh start`，通过`zkServer.sh status`查看角色（Leader/Follower）。

### 2. HBase 集群安装配置

#### 操作步骤

1. 下载 HBase 2.5.7 安装包，上传至 master 的 /opt/apps 目录。

2. 解压安装包到 /usr/local：`tar -zxvf hbase-2.5.7-bin.tar.gz -C /usr/local/`。

3. 配置环境变量：编辑 /etc/profile，添加`export HBASE_HOME=/usr/local/hbase-2.5.7`和`export PATH=$PATH:$HBASE_HOME/bin`，执行`source /etc/profile`生效。

4. 修改核心配置文件（位于 $HBASE_HOME/conf）：

   - 编辑 hbase-env.sh：添加 JDK 路径`export JAVA_HOME=/usr/java/jdk1.8.0_281-amd64`，禁用内置 ZooKeeper`export HBASE_MANAGES_ZK=false`。

   - 编辑 hbase-site.xml：添加 HDFS 连接、ZooKeeper 连接等配置：

     ```xml
     <configuration>
       <property>
         <name>hbase.rootdir</name>
         <value>hdfs://master:9000/hbase</value>
       </property>
       <property>
         <name>hbase.cluster.distributed</name>
         <value>true</value>
       </property>
       <property>
         <name>hbase.zookeeper.quorum</name>
         <value>slave1,slave2,slave3</value>
       </property>
       <property>
         <name>hbase.zookeeper.property.dataDir</name>
         <value>/usr/local/apache-zookeeper-3.8.3-bin/data</value>
       </property>
     </configuration>
     ```

     

   - 编辑 regionservers：删除默认内容，添加 HRegionServer 节点：

     ```plaintext
     slave1
     slave2
     slave3
     ```

     

5. 分发配置：将 master 的 HBase 目录与 /etc/profile 文件分发至 slave1、slave2、slave3，刷新环境变量。

6. 启动集群：

   - 先启动 Hadoop 集群：`start-all.sh`（在 master 节点执行）。
   - 启动 ZooKeeper 集群：在 slave1、slave2、slave3 分别执行`zkServer.sh start`。
   - 启动 HBase 集群：`start-hbase.sh`（在 master 节点执行）。

7. 验证集群：通过浏览器访问`http://master:16010`查看 HBase Master 状态，访问`http://slave1:16030`查看 RegionServer 状态。

## 五、HBase Shell 命令实操

HBase Shell 是操作 HBase 的命令行工具，支持表管理、数据操作等核心功能，以下是常用命令实操：

### 1. 表管理操作

```shell
# 1. 创建表（表名+列族，必须指定列族）
create 'film_RateNum', 'scoringtimes'

# 2. 查看所有表
list

# 3. 查看表结构详情
describe 'film_RateNum'

# 4. 禁用表（修改/删除表前必须执行）
disable 'film_RateNum'

# 5. 启用表
enable 'film_RateNum'

# 6. 检查表是否存在
exists 'film_RateNum'

# 7. 修改表（添加列族）
alter 'film_RateNum', NAME=>'info'

# 8. 修改表（删除列族）
alter 'film_RateNum', 'delete'=>'info'

# 9. 删除表（需先禁用）
disable 'film_RateNum'
drop 'film_RateNum'

# 10. 清空表数据
truncate 'film_RateNum'
```

### 2. 数据操作命令

```shell
# 1. 插入数据（表名、行键、列族:列限定符、值）
put 'film_RateNum', 'Top1', 'scoringtimes:MovieID', '1001'
put 'film_RateNum', 'Top1', 'scoringtimes:Movietype', 'Drama'
put 'film_RateNum', 'Top1', 'scoringtimes:RateNum', '1200'

# 2. 查询单行数据（表名、行键）
get 'film_RateNum', 'Top1'

# 3. 查询指定列族/列的数据
get 'film_RateNum', 'Top1', 'scoringtimes'
get 'film_RateNum', 'Top1', 'scoringtimes:MovieID'

# 4. 扫描表数据（全表扫描）
scan 'film_RateNum'

# 5. 扫描表数据（指定行键范围）
scan 'film_RateNum', {STARTROW=>'Top1', ENDROW=>'Top5'}

# 6. 统计表行数
count 'film_RateNum'

# 7. 删除指定单元格数据（表名、行键、列族:列限定符）
delete 'film_RateNum', 'Top1', 'scoringtimes:RateNum'

# 8. 删除整行数据
deleteall 'film_RateNum', 'Top1'
```

## 六、HBase Java API 实操

HBase 提供 Java API 用于编程开发，以下以 IntelliJ IDEA 为开发工具，实现电影影评数据存储实操：

### 1. 搭建开发环境

1. 创建 Maven 项目，在 pom.xml 中添加 HBase 依赖：

   ```xml
   <dependencies>
     <!-- HBase核心依赖 -->
     <dependency>
       <groupId>org.apache.hbase</groupId>
       <artifactId>hbase-client</artifactId>
       <version>2.5.7</version>
     </dependency>
     <dependency>
       <groupId>org.apache.hbase</groupId>
       <artifactId>hbase-common</artifactId>
       <version>2.5.7</version>
     </dependency>
     <dependency>
       <groupId>org.apache.hbase</groupId>
       <artifactId>hbase-server</artifactId>
       <version>2.5.7</version>
     </dependency>
     <!-- 单元测试依赖 -->
     <dependency>
       <groupId>junit</groupId>
       <artifactId>junit</artifactId>
       <version>4.12</version>
       <scope>test</scope>
     </dependency>
   </dependencies>
   ```

   

2. 复制 HBase 集群的 hbase-site.xml 文件到项目`src/main/resources`目录，确保客户端与集群配置一致。

### 2. 核心 API 工具类封装

创建 HBase 连接工具类，负责连接创建与关闭，避免重复编码：

```java
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.client.Admin;
import org.apache.hadoop.hbase.client.Connection;
import org.apache.hadoop.hbase.client.ConnectionFactory;
import org.apache.hadoop.conf.Configuration;
import java.io.IOException;

public class HBaseUtil {
    private static Configuration conf;
    private static Connection connection;

    static {
        // 初始化配置
        conf = HBaseConfiguration.create();
        try {
            // 创建连接（线程安全，全局单例）
            connection = ConnectionFactory.createConnection(conf);
        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    // 获取Connection连接
    public static Connection getConnection() {
        return connection;
    }

    // 获取Admin对象（用于表管理）
    public static Admin getAdmin() throws IOException {
        return connection.getAdmin();
    }

    // 关闭资源
    public static void close(Admin admin) {
        if (admin != null) {
            try {
                admin.close();
            } catch (IOException e) {
                e.printStackTrace();
            }
        }
    }
}
```

### 3. 表创建与数据插入实操

以 “存储评分次数最多的 10 部电影” 为例，实现表设计与数据存储：

#### （1）表结构设计

表名：film_RateNum，行键：Top1~Top10（对应评分排名），列族：scoringtimes，列限定符：MovieID（电影 ID）、Movietype（电影类型）、RateNum（评分次数）。

#### （2）Java 代码实现

```java
import org.apache.hadoop.hbase.HTableDescriptor;
import org.apache.hadoop.hbase.TableName;
import org.apache.hadoop.hbase.client.Admin;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.client.Table;
import org.apache.hadoop.hbase.io.compress.Compression;
import org.apache.hadoop.hbase.regionserver.BloomType;
import org.apache.hadoop.hbase.util.Bytes;
import java.io.IOException;

public class FilmRateNumDAO {
    // 表名与列族定义
    private static final TableName TABLE_NAME = TableName.valueOf("film_RateNum");
    private static final String CF_SCORING_TIMES = "scoringtimes";

    // 创建表
    public static void createTable() throws IOException {
        Admin admin = HBaseUtil.getAdmin();
        if (!admin.tableExists(TABLE_NAME)) {
            HTableDescriptor tableDesc = new HTableDescriptor(TABLE_NAME);
            // 添加列族，设置压缩方式与布隆过滤器
            tableDesc.addFamily(
                new HColumnDescriptor(CF_SCORING_TIMES)
                    .setCompressionType(Compression.Algorithm.SNAPPY)
                    .setBloomFilterType(BloomType.ROW)
            );
            admin.createTable(tableDesc);
            System.out.println("表film_RateNum创建成功！");
        } else {
            System.out.println("表film_RateNum已存在！");
        }
        HBaseUtil.close(admin);
    }

    // 插入数据
    public static void insertData(String rowKey, String movieId, String movieType, String rateNum) throws IOException {
        Table table = HBaseUtil.getConnection().getTable(TABLE_NAME);
        Put put = new Put(Bytes.toBytes(rowKey));
        // 向列族添加数据（列限定符+值）
        put.addColumn(
            Bytes.toBytes(CF_SCORING_TIMES),
            Bytes.toBytes("MovieID"),
            Bytes.toBytes(movieId)
        );
        put.addColumn(
            Bytes.toBytes(CF_SCORING_TIMES),
            Bytes.toBytes("Movietype"),
            Bytes.toBytes(movieType)
        );
        put.addColumn(
            Bytes.toBytes(CF_SCORING_TIMES),
            Bytes.toBytes("RateNum"),
            Bytes.toBytes(rateNum)
        );
        table.put(put);
        table.close();
        System.out.println("行键" + rowKey + "数据插入成功！");
    }

    // 主函数测试
    public static void main(String[] args) throws IOException {
        // 创建表
        createTable();
        // 插入测试数据（Top1~Top3）
        insertData("Top1", "2858", "Drama", "1560");
        insertData("Top2", "1196", "Adventure", "1480");
        insertData("Top3", "1210", "Fantasy", "1420");
    }
}
```

### 4. 数据查询实操

添加查询方法，实现单行查询与全表扫描：

```java
import org.apache.hadoop.hbase.client.Get;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.client.ResultScanner;
import org.apache.hadoop.hbase.client.Scan;
import org.apache.hadoop.hbase.util.Bytes;
import java.io.IOException;

public class FilmRateNumDAO {
    // 省略创建表、插入数据方法...

    // 单行查询
    public static void queryByRowKey(String rowKey) throws IOException {
        Table table = HBaseUtil.getConnection().getTable(TABLE_NAME);
        Get get = new Get(Bytes.toBytes(rowKey));
        Result result = table.get(get);
        // 解析查询结果
        String movieId = Bytes.toString(result.getValue(Bytes.toBytes(CF_SCORING_TIMES), Bytes.toBytes("MovieID")));
        String movieType = Bytes.toString(result.getValue(Bytes.toBytes(CF_SCORING_TIMES), Bytes.toBytes("Movietype")));
        String rateNum = Bytes.toString(result.getValue(Bytes.toBytes(CF_SCORING_TIMES), Bytes.toBytes("RateNum")));
        System.out.println("行键：" + rowKey + "，电影ID：" + movieId + "，类型：" + movieType + "，评分次数：" + rateNum);
        table.close();
    }

    // 全表扫描
    public static void scanTable() throws IOException {
        Table table = HBaseUtil.getConnection().getTable(TABLE_NAME);
        Scan scan = new Scan();
        ResultScanner scanner = table.getScanner(scan);
        // 遍历扫描结果
        for (Result result : scanner) {
            String rowKey = Bytes.toString(result.getRow());
            String movieId = Bytes.toString(result.getValue(Bytes.toBytes(CF_SCORING_TIMES), Bytes.toBytes("MovieID")));
            String movieType = Bytes.toString(result.getValue(Bytes.toBytes(CF_SCORING_TIMES), Bytes.toBytes("Movietype")));
            String rateNum = Bytes.toString(result.getValue(Bytes.toBytes(CF_SCORING_TIMES), Bytes.toBytes("RateNum")));
            System.out.println("行键：" + rowKey + "，电影ID：" + movieId + "，类型：" + movieType + "，评分次数：" + rateNum);
        }
        scanner.close();
        table.close();
    }

    // 主函数测试查询
    public static void main(String[] args) throws IOException {
        // 单行查询
        queryByRowKey("Top1");
        // 全表扫描
        System.out.println("全表数据：");
        scanTable();
    }
}
```

## 七、电影影评分析结果存储项目实操

### 1. 项目需求

将 4 类电影影评分析结果存储到 HBase，分别设计 4 张表，对应不同数据类型：

| 数据类型                         | 表名               | 行键设计                        | 列族         | 列限定符                    |
| -------------------------------- | ------------------ | ------------------------------- | ------------ | --------------------------- |
| 评分次数 Top10 电影              | film_RateNum       | Top1~Top10                      | scoringtimes | MovieID、Movietype、RateNum |
| 不同性别评分 Top10 电影          | film_GenderRatings | 性别_Top1~Top10（如 Male_Top1） | info         | MovieID、Rating             |
| 指定电影（2858）各年龄段平均评分 | film_AgeRatings    | Age = 年龄段（如 Age=18）       | info         | avgRating、MovieID          |
| 各类型评分 Top5 电影             | film_TypeRatings   | 类型_Top1~Top5（如 Drama_Top1） | info         | MovieID、Rating             |

### 2. 核心实现步骤（以 film_GenderRatings 为例）

#### （1）表创建

```java
public class FilmGenderRatingsDAO {
    private static final TableName TABLE_NAME = TableName.valueOf("film_GenderRatings");
    private static final String CF_INFO = "info";

    // 创建表
    public static void createTable() throws IOException {
        Admin admin = HBaseUtil.getAdmin();
        if (!admin.tableExists(TABLE_NAME)) {
            HTableDescriptor tableDesc = new HTableDescriptor(TABLE_NAME);
            tableDesc.addFamily(new HColumnDescriptor(CF_INFO));
            admin.createTable(tableDesc);
            System.out.println("表film_GenderRatings创建成功！");
        }
        HBaseUtil.close(admin);
    }
}
```

#### （2）读取 HDFS 数据并插入 HBase

```java
import org.apache.hadoop.fs.FSDataInputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import java.io.BufferedReader;
import java.io.InputStreamReader;

public class FilmGenderRatingsDAO {
    // 省略表创建方法...

    // 从HDFS读取数据并插入HBase
    public static void insertDataFromHDFS(String hdfsPath) throws IOException {
        Table table = HBaseUtil.getConnection().getTable(TABLE_NAME);
        // 获取HDFS文件系统
        FileSystem fs = FileSystem.get(HBaseUtil.getConnection().getConfiguration());
        FSDataInputStream in = fs.open(new Path(hdfsPath));
        BufferedReader br = new BufferedReader(new InputStreamReader(in));
        String line;
        int rank = 1;
        // 逐行读取数据（格式：性别 电影ID 评分）
        while ((line = br.readLine()) != null) {
            String[] fields = line.split("\t");
            String gender = fields[0];
            String movieId = fields[1];
            String rating = fields[2];
            // 构建行键（如Male_Top1）
            String rowKey = gender + "_Top" + rank;
            // 插入数据
            Put put = new Put(Bytes.toBytes(rowKey));
            put.addColumn(Bytes.toBytes(CF_INFO), Bytes.toBytes("MovieID"), Bytes.toBytes(movieId));
            put.addColumn(Bytes.toBytes(CF_INFO), Bytes.toBytes("Rating"), Bytes.toBytes(rating));
            table.put(put);
            rank++;
            if (rank > 10) rank = 1; // 每个性别取Top10
        }
        br.close();
        in.close();
        table.close();
        System.out.println("HDFS数据插入HBase成功！");
    }

    // 主函数测试
    public static void main(String[] args) throws IOException {
        createTable();
        // HDFS数据路径（影评分析结果存储路径）
        String hdfsPath = "/join/MoviesRatesTop10GroupByGender/part-r-00000";
        insertDataFromHDFS(hdfsPath);
    }
}
```

## 八、常见问题与排查

1. HBase 启动失败，提示 “ZooKeeper connection timeout”：检查 ZooKeeper 集群是否启动，hbase-site.xml 中 zookeeper.quorum 配置是否正确。
2. Java API 连接 HBase 报错 “Connection refused”：确保 HBase 集群正常运行，防火墙已关闭，客户端与集群节点网络互通。
3. 插入数据后查询无结果：检查行键、列族、列限定符是否与插入时一致，HBase 表是否已启用。
4. HBase Master 启动后自动停止：查看日志（$HBASE_HOME/logs），可能是 Hadoop 集群未启动或 HDFS 权限不足，执行`hdfs dfs -chmod 777 /hbase`授权。

## 九、实操总结

本教程从 HBase 基础认知出发，逐步推进至分布式环境搭建、Shell 命令操作、Java API 开发，最终落地电影影评分析结果存储项目。核心要点包括：HBase 集群依赖 ZooKeeper 实现高可用，表设计需重点关注行键与列族规划，Java API 操作需封装连接工具类提高效率。实际应用中，可根据数据特点调整表结构与配置，优化存储性能。

要不要我帮你整理一份**HBase 实操核心资源包**，包含集群配置文件模板、Shell 命令速查表、Java API 工具类完整代码，方便你快速复用？