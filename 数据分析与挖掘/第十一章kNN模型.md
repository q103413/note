
第十一章kNN模型的应用

01 kNN算法（K最近邻算法）的思想
	核心思想
		比较已知y值的样本与未知y值样本的相似度，然后寻找最相似的k个样本用作未知样本的预测。
	定义
		如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。
		来源：KNN算法最早是由Cover和Hart提出的一种分类算法
	
	算法步骤
		确定未知样本近邻的个数k值。		根据某种度量样本间相似度的指标（如欧氏距离）将每一个未知类别样本的最近k个已知样本搜寻出来，形成一个个簇。		对搜寻出来的已知样本进行投票，将各簇下类别最多的分类用作未知样本点的预测。02 最佳k值的选择
	    k值对模型的影响
			如果k值偏小，过拟合；反之，欠拟合。
		
		k值选择方案
			一种是设置k近邻样本的投票权重
				通常可以将权重设置为距离的倒数。
				
			多重交叉验证法
				核心就是将k取不同的值，然后在每种值下执行m重的交叉验证，最后选出平均误差最小的k值。
				03 样本间相似度的度量方法
	    欧氏距离
			d_A,B=√(y_1−x_1)^2+(y_2−x_2)^2+⋯+(y_n−x_n)^2
			
		曼哈顿距离
			d_A,B=|y_1−x_1|+|y_2−x_2|+…+|y_n−x_n|
		
		余弦相似度
		杰卡德相似系数
04 KNN算法的应用实战	neighbors.KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', 			           leaf_size=30, p=2, metric='minkowski', 			           metric_params=None, n_jobs=1)	n_neighbors：用于指定近邻样本个数K，默认为5
	weights：用于指定近邻样本的投票权重，默认为'uniform'，表示所有近邻样本的投票权重一样；如果为'distance'，则表示投票权重与距离成反比，即近邻样本与未知类别的样本点距离越远，权重越小，反之，权重越大
	metric：用于指定距离的度量指标，默认为闵可夫斯基距离	p：当参数metric为闵可夫斯基距离时，p=1，表示计算点之间的曼哈顿距离；p=2，表示计算点之间的欧氏距离；该参数的默认值为2	
数据准备	
	导入第三方模块（import）
	读取数据 （pd.read_csv）
	数据清洗
		探索性分析（连续型数据/离散型数据）
		数据可视化 
		查看数据集是否存在缺失值
		缺失值处理(删除/填补)
			删除缺失(如缺值的样本占总数比例极高的情况)
			填补缺失（常数替换、前/后向替换、统计值替换）
特征工程（对不同的数据类型的特征提取）
	判断是否做标准化处理（量纲不一致）
	连续变量的处理（无量纲化）
	离散变量的处理
		1）有直接类别的
			重编码pd.Categorical(list).codes 
			哑变量pd.get_dummies()
		2）字符串类型：先提取特征，再归到分类数据中（必须把数据转化为分类数据）
	时间序列
	删除无关变量
建模
	数据拆分（train_test_split）
	构建模型
		导入算法（先选择模型）
		找出最佳K值
			使用多重交叉验证的方法，找出最佳K值(model_selection.cross_val_score)
		创建模型
		训练模型(训练集）
	模型预测（测试集）
		model.predict(X_test)
	评估模型(先导入metrics模块)
		连续型数值的预测：
			计算MSE值
		离散型变量的分类：
			准确率（model.score或者metrics.accuracy_score）
			混淆矩阵
				confusion_matrix
				pd.crosstab(predict,y_test)
			ROC曲线、
			热力图（sns.heatmap）
	
		

			
			